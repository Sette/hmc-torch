{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ecb52a-eafe-48b6-a816-2582938ab995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5a6ff1-75a6-45af-a954-8f14836faa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# from hmc.utils import create_dir, __load_json__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fce6de2-7e00-4c5a-ac04-04eb1e4b4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dir(path):\n",
    "    # checking if the directory demo_folder2\n",
    "    # exist or not.\n",
    "    if not os.path.isdir(path):\n",
    "        # if the demo_folder2 directory is\n",
    "        # not present then create it.\n",
    "        os.makedirs(path)\n",
    "    return True\n",
    "\n",
    "\n",
    "def __load_json__(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        tmp = json.loads(f.read())\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705f4c8f-487e-4876-87c0-34ee6e995858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53de739-29ea-4bf5-bb90-8921e89a63d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 18:14:55.153483: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-29 18:14:55.245718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756502095.468403  898493 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756502095.509607  898493 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756502095.708802  898493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756502095.708829  898493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756502095.708831  898493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756502095.708833  898493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-29 18:14:55.722152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7016715-d3a4-41f0-b759-497faa935c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/bruno/git/hmc-torch/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f344b32e-b77b-479c-a181-0f389195fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arff_path = os.path.join(data_path, 'HMC_data_arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2630463a-a73a-4ca5-bfc3-4bb8e3e135c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fun_path = os.path.join(dataset_arff_path, 'datasets_FUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ec8924-8f16-46c3-9985-4e6aaab6effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_go_path = os.path.join(dataset_arff_path, 'datasets_GO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a31001-fb52-4243-aa91-40398d1183a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_others_path = os.path.join(dataset_arff_path, 'others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9e4ff68-230e-4084-8a2b-053016db3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'enron_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'Enron_corr_trainvalid.arff'),\n",
    "        os.path.join(dataset_others_path, 'Enron_corr_test.arff')\n",
    "    ),\n",
    "    'diatoms_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'Diatoms_train.arff'),\n",
    "        os.path.join(dataset_others_path, 'Diatoms_test.arff')\n",
    "    ),\n",
    "    'imclef07a_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07A_Train.arff'),\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07A_Test.arff')\n",
    "    ),\n",
    "    'imclef07d_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07D_Train.arff'),\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07D_Test.arff')\n",
    "    ),\n",
    "    'cellcycle_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'cellcycle_FUN/cellcycle_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'cellcycle_FUN/cellcycle_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'cellcycle_FUN/cellcycle_FUN.valid.arff'),\n",
    "    ),\n",
    "    'derisi_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'derisi_FUN/derisi_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'derisi_FUN/derisi_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'derisi_FUN/derisi_FUN.valid.arff'),\n",
    "    ),\n",
    "    'eisen_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'eisen_FUN/eisen_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'eisen_FUN/eisen_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'eisen_FUN/eisen_FUN.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'expr_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'expr_FUN/expr_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'expr_FUN/expr_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'expr_FUN/expr_FUN.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'gasch1_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'gasch1_FUN/gasch1_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch1_FUN/gasch1_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch1_FUN/gasch1_FUN.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'gasch2_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'gasch2_FUN/gasch2_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch2_FUN/gasch2_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch2_FUN/gasch2_FUN.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'seq_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'seq_FUN/seq_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'seq_FUN/seq_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'seq_FUN/seq_FUN.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'spo_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'spo_FUN/spo_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'spo_FUN/spo_FUN.test.arff'),\n",
    "        os.path.join(dataset_fun_path, 'spo_FUN/spo_FUN.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'cellcycle_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'cellcycle_GO/cellcycle_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'cellcycle_GO/cellcycle_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'cellcycle_GO/cellcycle_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'derisi_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'derisi_GO/derisi_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'derisi_GO/derisi_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'derisi_GO/derisi_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'eisen_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'eisen_GO/eisen_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'eisen_GO/eisen_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'eisen_GO/eisen_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'expr_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'expr_GO/expr_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'expr_GO/expr_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'expr_GO/expr_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'gasch1_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'gasch1_GO/gasch1_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch1_GO/gasch1_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch1_GO/gasch1_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'gasch2_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'gasch2_GO/gasch2_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch2_GO/gasch2_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch2_GO/gasch2_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'seq_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'seq_GO/seq_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'seq_GO/seq_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'seq_GO/seq_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "    'spo_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'spo_GO/spo_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'spo_GO/spo_GO.test.arff'),\n",
    "        os.path.join(dataset_go_path, 'spo_GO/spo_GO.valid.arff'),\n",
    "        \n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "385a1b0e-cd4c-41c2-90aa-1079ea2bb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_example(data):\n",
    "    features, labels = data\n",
    "    example = {\n",
    "        'features': features,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e1238fc-27da-4875-a91e-2c81d084df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset_arff_tocsv(name, datasets, output_path):\n",
    "    dataset = datasets[name]\n",
    "\n",
    "    is_GO = dataset[0]\n",
    "    train = dataset[1]\n",
    "    test = dataset[2]\n",
    "\n",
    "    train = arff_data_to_df(train, is_GO, output_path)\n",
    "    test = arff_data_to_df(test, is_GO, output_path)\n",
    "    \n",
    "    if len(dataset) == 4:\n",
    "        val = dataset[3]\n",
    "        val = arff_data_to_csv(val, is_GO, output_path)\n",
    "        return train, test, val\n",
    "\n",
    "    return train, test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f11707f1-09df-4ffd-bfb9-c081599e286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class arff_data_to_df():\n",
    "    def __init__(self, arff_file,  is_GO, output_path):\n",
    "        self.arrf_file = arff_file\n",
    "        self.output_path = output_path\n",
    "        create_dir(self.output_path)\n",
    "        self.csv_file = '/'.join(arff_file.split('/')[-3:]).replace('.arff', '.csv')\n",
    "        self.X, self.Y = parse_arff_to_df(arff_file=arff_file, output_path=self.output_path , is_GO=is_GO)\n",
    "        r_, c_ = np.where(np.isnan(self.X))\n",
    "        m = np.nanmean(self.X, axis=0)\n",
    "        for i, j in zip(r_, c_):\n",
    "            self.X[i][j] = m[j]\n",
    "\n",
    "    def to_csv(self, output_file):\n",
    "        \"\"\"Salva X e Y como um arquivo CSV.\"\"\"\n",
    "        # Criando DataFrame para X\n",
    "        output_path = '/'.join(output_file.split('/')[:-1])\n",
    "        create_dir(output_path)\n",
    "\n",
    "        df_X = pd.DataFrame({'features': [json.dumps(x) for x in self.X]})\n",
    "        #df_X = pd.DataFrame({'features': json.dumps(self.X)})\n",
    "        #df_X = pd.DataFrame({'features': json.dumps(self.X)})\n",
    "        # Criando DataFrame para Y, convertendo para int se necessário\n",
    "        df_Y = pd.DataFrame({'labels':self.Y})\n",
    "\n",
    "        # Concatenando X e Y\n",
    "        df = pd.concat([df_X, df_Y], axis=1)\n",
    "\n",
    "        # Salvando como CSV\n",
    "        df.to_csv(output_file, sep='|' , index=False)\n",
    "        print(f\"CSV salvo em: {output_file}\")\n",
    "\n",
    "\n",
    "    def to_pt(self, output_path):\n",
    "        \"\"\"Salva X e Y como um arquivo pt.\"\"\"\n",
    "        # Criando DataFrame para X\n",
    "        create_dir(output_path)\n",
    "        batch_size = 1024 * 50  # 50k records from each file batch\n",
    "        count = 0\n",
    "        total = math.ceil(len(self.X) / batch_size)\n",
    "        for i in range(0, len(self.X), batch_size):\n",
    "            batch_X = self.X[i:i + batch_size]\n",
    "            batch_Y = self.Y[i:i + batch_size]\n",
    "            pt_records = [create_example(data) for data in zip(batch_X, batch_Y)]\n",
    "            path = f\"{output_path}/{str(count).zfill(10)}.pt\"\n",
    "\n",
    "            torch.save(pt_records, path)\n",
    "\n",
    "            print(f\"{count} {len(pt_records)} {path}\")\n",
    "            count += 1\n",
    "            print(f\"{count}/{total} batches / {count * batch_size} processed\")\n",
    "\n",
    "        print(f\"{count}/{total} batches / {len(self.X)} processed\")\n",
    "\n",
    "\n",
    "def parse_arff_to_df(arff_file, output_path, is_GO=False):\n",
    "    with open(arff_file) as f:\n",
    "        read_data = False\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        feature_types = []\n",
    "        d = []\n",
    "        cats_lens = []\n",
    "        all_terms = []\n",
    "        for num_line, l in enumerate(f):\n",
    "            if l.startswith('@ATTRIBUTE'):\n",
    "                if l.startswith('@ATTRIBUTE class'):\n",
    "                    h = l.split('hierarchical')[1].strip()\n",
    "                    for branch in h.split(','):\n",
    "                        branch = branch.replace('/', '.')\n",
    "                        all_terms.append(branch)\n",
    "\n",
    "                else:\n",
    "                    _, f_name, f_type = l.split()\n",
    "\n",
    "                    if f_type == 'numeric' or f_type == 'NUMERIC':\n",
    "                        d.append([])\n",
    "                        cats_lens.append(1)\n",
    "                        feature_types.append(lambda x, i: [float(x)] if x != '?' else [np.nan])\n",
    "\n",
    "                    else:\n",
    "                        cats = f_type[1:-1].split(',')\n",
    "                        cats_lens.append(len(cats))\n",
    "                        d.append({key: keras.utils.to_categorical(i, len(cats)).tolist() for i, key in enumerate(cats)})\n",
    "                        feature_types.append(lambda x, i: d[i].get(x, [0.0] * cats_lens[i]))\n",
    "            elif l.startswith('@DATA'):\n",
    "                read_data = True\n",
    "            elif read_data:\n",
    "                d_line = l.split('%')[0].strip().split(',')\n",
    "                lab = d_line[len(feature_types)].replace('/', '.').strip()\n",
    "\n",
    "                X.append(list(chain(*[feature_types[i](x, i) for i, x in enumerate(d_line[:len(feature_types)])])))\n",
    "\n",
    "                #for t in lab.split('@'):\n",
    "                #    y_[[nodes_idx.get(a) for a in nx.ancestors(g_t, t.replace('/', '.'))]] = 1\n",
    "                #    y_[nodes_idx[t.replace('/', '.')]] = 1\n",
    "                Y.append(lab)\n",
    "        #X = np.array(X)\n",
    "        #Y = np.stack(Y)\n",
    "        categories = {\"labels\": all_terms}\n",
    "        if \"train\" in arff_file or \"Train\" in arff_file:\n",
    "            create_dir(output_path)\n",
    "            labels_file = os.path.join(output_path, \"labels.json\")\n",
    "            with open(labels_file, \"w+\") as f:\n",
    "                f.write(json.dumps(categories))\n",
    "        #np.save('all_terms.npy', np.array(all_terms))\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3eb0242-5e46-4ac6-bb79-63542aa5e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_dump = json.dumps(train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12666c9d-7247-4198-9d1b-eb897bceee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['features'] = df['features'].apply(json.loads)  # Agora funcionará\n",
    "\n",
    "# X = df['features'].tolist()  # Se quiser lista\n",
    "# # ou diretamente em NumPy\n",
    "# X = np.array(df['features'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8320ea56-2d02-487d-ba27-ff1a0117ece5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dbfb3bf7394b0ca0ae7bf2a00be6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: enron_others\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/enron_others/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/enron_others/test.csv\n",
      "Processing dataset: diatoms_others\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/diatoms_others/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/diatoms_others/test.csv\n",
      "Processing dataset: imclef07a_others\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/imclef07a_others/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/imclef07a_others/test.csv\n",
      "Processing dataset: imclef07d_others\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/imclef07d_others/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/imclef07d_others/test.csv\n",
      "Processing dataset: cellcycle_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/cellcycle_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/cellcycle_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/cellcycle_FUN/val.csv\n",
      "Processing dataset: derisi_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/derisi_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/derisi_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/derisi_FUN/val.csv\n",
      "Processing dataset: eisen_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/eisen_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/eisen_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/eisen_FUN/val.csv\n",
      "Processing dataset: expr_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/expr_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/expr_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/expr_FUN/val.csv\n",
      "Processing dataset: gasch1_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch1_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch1_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch1_FUN/val.csv\n",
      "Processing dataset: gasch2_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch2_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch2_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch2_FUN/val.csv\n",
      "Processing dataset: seq_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/seq_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/seq_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/seq_FUN/val.csv\n",
      "Processing dataset: spo_FUN\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/spo_FUN/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/spo_FUN/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/spo_FUN/val.csv\n",
      "Processing dataset: cellcycle_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/cellcycle_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/cellcycle_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/cellcycle_GO/val.csv\n",
      "Processing dataset: derisi_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/derisi_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/derisi_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/derisi_GO/val.csv\n",
      "Processing dataset: eisen_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/eisen_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/eisen_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/eisen_GO/val.csv\n",
      "Processing dataset: expr_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/expr_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/expr_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/expr_GO/val.csv\n",
      "Processing dataset: gasch1_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch1_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch1_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch1_GO/val.csv\n",
      "Processing dataset: gasch2_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch2_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch2_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/gasch2_GO/val.csv\n",
      "Processing dataset: seq_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/seq_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/seq_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/seq_GO/val.csv\n",
      "Processing dataset: spo_GO\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/spo_GO/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/spo_GO/test.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/spo_GO/val.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# In[35]:\n",
    "for dataset_name in tqdm(datasets.keys()):\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "    # dataset_name = 'seq_FUN'\n",
    "\n",
    "    # In[36]:\n",
    "\n",
    "    output_path = os.path.join(data_path, \"HMC_data_csv\", dataset_name)\n",
    "\n",
    "    # In[38]:\n",
    "\n",
    "    dataset = initialize_dataset_arff_tocsv(dataset_name, datasets, output_path)\n",
    "\n",
    "    train = dataset[0]\n",
    "    test = dataset[1]\n",
    "\n",
    "    train.to_csv(os.path.join(output_path, \"train.csv\"))\n",
    "    test.to_csv(os.path.join(output_path, \"test.csv\"))\n",
    "    if len(dataset) == 3:\n",
    "        val = dataset[2]\n",
    "        val.to_csv(os.path.join(output_path, \"val.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e04e5-8839-428a-8a93-e9499d69b1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d6539-970b-43bf-95b0-fc33c0d00ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ff16d-1bf0-4cd4-ad2e-a14cea91d07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb70abd-e02a-492e-ba71-901617f3917e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
