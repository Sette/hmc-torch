{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ecb52a-eafe-48b6-a816-2582938ab995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5a6ff1-75a6-45af-a954-8f14836faa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "\n",
    "# from hmc.utils import create_dir, __load_json__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fce6de2-7e00-4c5a-ac04-04eb1e4b4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dir(path):\n",
    "    # checking if the directory demo_folder2\n",
    "    # exist or not.\n",
    "    if not os.path.isdir(path):\n",
    "        # if the demo_folder2 directory is\n",
    "        # not present then create it.\n",
    "        os.makedirs(path)\n",
    "    return True\n",
    "\n",
    "\n",
    "def __load_json__(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        tmp = json.loads(f.read())\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705f4c8f-487e-4876-87c0-34ee6e995858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53de739-29ea-4bf5-bb90-8921e89a63d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 18:36:31.350680: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-27 18:36:31.467314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756330591.714680 1749760 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756330591.758915 1749760 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756330592.188694 1749760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756330592.188728 1749760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756330592.188730 1749760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756330592.188732 1749760 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-27 18:36:32.200127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7016715-d3a4-41f0-b759-497faa935c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/bruno/git/hmc-torch/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f344b32e-b77b-479c-a181-0f389195fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arff_path = os.path.join(data_path, 'HMC_data_arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2630463a-a73a-4ca5-bfc3-4bb8e3e135c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fun_path = os.path.join(dataset_arff_path, 'datasets_FUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ec8924-8f16-46c3-9985-4e6aaab6effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_go_path = os.path.join(dataset_arff_path, 'datasets_GO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a31001-fb52-4243-aa91-40398d1183a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_others_path = os.path.join(dataset_arff_path, 'others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9e4ff68-230e-4084-8a2b-053016db3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'enron_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'Enron_corr_trainvalid.arff'),\n",
    "        os.path.join(dataset_others_path, 'Enron_corr_test.arff')\n",
    "    ),\n",
    "    'diatoms_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'Diatoms_train.arff'),\n",
    "        os.path.join(dataset_others_path, 'Diatoms_test.arff')\n",
    "    ),\n",
    "    'imclef07a_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07A_Train.arff'),\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07A_Test.arff')\n",
    "    ),\n",
    "    'imclef07d_others': (\n",
    "        False,\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07D_Train.arff'),\n",
    "        os.path.join(dataset_others_path, 'ImCLEF07D_Test.arff')\n",
    "    ),\n",
    "    'cellcycle_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'cellcycle_FUN/cellcycle_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'cellcycle_FUN/cellcycle_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'cellcycle_FUN/cellcycle_FUN.test.arff')\n",
    "    ),\n",
    "    'derisi_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'derisi_FUN/derisi_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'derisi_FUN/derisi_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'derisi_FUN/derisi_FUN.test.arff')\n",
    "    ),\n",
    "    'eisen_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'eisen_FUN/eisen_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'eisen_FUN/eisen_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'eisen_FUN/eisen_FUN.test.arff')\n",
    "    ),\n",
    "    'expr_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'expr_FUN/expr_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'expr_FUN/expr_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'expr_FUN/expr_FUN.test.arff')\n",
    "    ),\n",
    "    'gasch1_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'gasch1_FUN/gasch1_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch1_FUN/gasch1_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch1_FUN/gasch1_FUN.test.arff')\n",
    "    ),\n",
    "    'gasch2_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'gasch2_FUN/gasch2_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch2_FUN/gasch2_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'gasch2_FUN/gasch2_FUN.test.arff')\n",
    "    ),\n",
    "    'seq_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'seq_FUN/seq_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'seq_FUN/seq_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'seq_FUN/seq_FUN.test.arff')\n",
    "    ),\n",
    "    'spo_FUN': (\n",
    "        False,\n",
    "        os.path.join(dataset_fun_path, 'spo_FUN/spo_FUN.train.arff'),\n",
    "        os.path.join(dataset_fun_path, 'spo_FUN/spo_FUN.valid.arff'),\n",
    "        os.path.join(dataset_fun_path, 'spo_FUN/spo_FUN.test.arff')\n",
    "    ),\n",
    "    'cellcycle_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'cellcycle_GO/cellcycle_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'cellcycle_GO/cellcycle_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'cellcycle_GO/cellcycle_GO.test.arff')\n",
    "    ),\n",
    "    'derisi_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'derisi_GO/derisi_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'derisi_GO/derisi_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'derisi_GO/derisi_GO.test.arff')\n",
    "    ),\n",
    "    'eisen_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'eisen_GO/eisen_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'eisen_GO/eisen_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'eisen_GO/eisen_GO.test.arff')\n",
    "    ),\n",
    "    'expr_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'expr_GO/expr_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'expr_GO/expr_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'expr_GO/expr_GO.test.arff')\n",
    "    ),\n",
    "    'gasch1_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'gasch1_GO/gasch1_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch1_GO/gasch1_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch1_GO/gasch1_GO.test.arff')\n",
    "    ),\n",
    "    'gasch2_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'gasch2_GO/gasch2_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch2_GO/gasch2_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'gasch2_GO/gasch2_GO.test.arff')\n",
    "    ),\n",
    "    'seq_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'seq_GO/seq_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'seq_GO/seq_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'seq_GO/seq_GO.test.arff')\n",
    "    ),\n",
    "    'spo_GO': (\n",
    "        True,\n",
    "        os.path.join(dataset_go_path, 'spo_GO/spo_GO.train.arff'),\n",
    "        os.path.join(dataset_go_path, 'spo_GO/spo_GO.valid.arff'),\n",
    "        os.path.join(dataset_go_path, 'spo_GO/spo_GO.test.arff')\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "385a1b0e-cd4c-41c2-90aa-1079ea2bb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_example(data):\n",
    "    features, labels = data\n",
    "    example = {\n",
    "        'features': features,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f11707f1-09df-4ffd-bfb9-c081599e286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class arff_data_to_csv():\n",
    "    def __init__(self, arff_file,  is_GO, output_path):\n",
    "        self.arrf_file = arff_file\n",
    "        self.output_path = output_path\n",
    "        create_dir(self.output_path)\n",
    "        self.csv_file = '/'.join(arff_file.split('/')[-3:]).replace('.arff', '.csv')\n",
    "        self.X, self.Y = parse_arff_to_csv(arff_file=arff_file, output_path=self.output_path , is_GO=is_GO)\n",
    "        r_, c_ = np.where(np.isnan(self.X))\n",
    "        m = np.nanmean(self.X, axis=0)\n",
    "        for i, j in zip(r_, c_):\n",
    "            self.X[i][j] = m[j]\n",
    "\n",
    "    def to_csv(self, output_file):\n",
    "        \"\"\"Salva X e Y como um arquivo CSV.\"\"\"\n",
    "        # Criando DataFrame para X\n",
    "        output_path = '/'.join(output_file.split('/')[:-1])\n",
    "        create_dir(output_path)\n",
    "\n",
    "        df_X = pd.DataFrame({'features': [json.dumps(x) for x in self.X]})\n",
    "        #df_X = pd.DataFrame({'features': json.dumps(self.X)})\n",
    "        #df_X = pd.DataFrame({'features': json.dumps(self.X)})\n",
    "        # Criando DataFrame para Y, convertendo para int se necessário\n",
    "        df_Y = pd.DataFrame({'labels':self.Y})\n",
    "\n",
    "        # Concatenando X e Y\n",
    "        df = pd.concat([df_X, df_Y], axis=1)\n",
    "\n",
    "        # Salvando como CSV\n",
    "        df.to_csv(output_file, sep='|' , index=False)\n",
    "        print(f\"CSV salvo em: {output_file}\")\n",
    "\n",
    "\n",
    "    def to_pt(self, output_path):\n",
    "        \"\"\"Salva X e Y como um arquivo pt.\"\"\"\n",
    "        # Criando DataFrame para X\n",
    "        create_dir(output_path)\n",
    "        batch_size = 1024 * 50  # 50k records from each file batch\n",
    "        count = 0\n",
    "        total = math.ceil(len(self.X) / batch_size)\n",
    "        for i in range(0, len(self.X), batch_size):\n",
    "            batch_X = self.X[i:i + batch_size]\n",
    "            batch_Y = self.Y[i:i + batch_size]\n",
    "            pt_records = [create_example(data) for data in zip(batch_X, batch_Y)]\n",
    "            path = f\"{output_path}/{str(count).zfill(10)}.pt\"\n",
    "\n",
    "            torch.save(pt_records, path)\n",
    "\n",
    "            print(f\"{count} {len(pt_records)} {path}\")\n",
    "            count += 1\n",
    "            print(f\"{count}/{total} batches / {count * batch_size} processed\")\n",
    "\n",
    "        print(f\"{count}/{total} batches / {len(self.X)} processed\")\n",
    "\n",
    "\n",
    "def parse_arff_to_csv(arff_file, output_path, is_GO=False):\n",
    "    with open(arff_file) as f:\n",
    "        read_data = False\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        feature_types = []\n",
    "        d = []\n",
    "        cats_lens = []\n",
    "        all_terms = []\n",
    "        for num_line, l in enumerate(f):\n",
    "            if l.startswith('@ATTRIBUTE'):\n",
    "                if l.startswith('@ATTRIBUTE class'):\n",
    "                    h = l.split('hierarchical')[1].strip()\n",
    "                    for branch in h.split(','):\n",
    "                        branch = branch.replace('/', '.')\n",
    "                        all_terms.append(branch)\n",
    "\n",
    "                else:\n",
    "                    _, f_name, f_type = l.split()\n",
    "\n",
    "                    if f_type == 'numeric' or f_type == 'NUMERIC':\n",
    "                        d.append([])\n",
    "                        cats_lens.append(1)\n",
    "                        feature_types.append(lambda x, i: [float(x)] if x != '?' else [np.nan])\n",
    "\n",
    "                    else:\n",
    "                        cats = f_type[1:-1].split(',')\n",
    "                        cats_lens.append(len(cats))\n",
    "                        d.append({key: keras.utils.to_categorical(i, len(cats)).tolist() for i, key in enumerate(cats)})\n",
    "                        feature_types.append(lambda x, i: d[i].get(x, [0.0] * cats_lens[i]))\n",
    "            elif l.startswith('@DATA'):\n",
    "                read_data = True\n",
    "            elif read_data:\n",
    "                d_line = l.split('%')[0].strip().split(',')\n",
    "                lab = d_line[len(feature_types)].replace('/', '.').strip()\n",
    "\n",
    "                X.append(list(chain(*[feature_types[i](x, i) for i, x in enumerate(d_line[:len(feature_types)])])))\n",
    "\n",
    "                #for t in lab.split('@'):\n",
    "                #    y_[[nodes_idx.get(a) for a in nx.ancestors(g_t, t.replace('/', '.'))]] = 1\n",
    "                #    y_[nodes_idx[t.replace('/', '.')]] = 1\n",
    "                Y.append(lab)\n",
    "        #X = np.array(X)\n",
    "        #Y = np.stack(Y)\n",
    "        categories = {'labels': all_terms}\n",
    "        if 'train' in arff_file:\n",
    "            labels_path = '/'.join(arff_file.split('/')[:-1])\n",
    "            dataset_path =  os.path.join(output_path, '/'.join(labels_path.split('/')[-2:]))\n",
    "            create_dir(dataset_path)\n",
    "            labels_file = os.path.join(dataset_path, 'labels.json')\n",
    "            with open(labels_file, 'w+') as f:\n",
    "                f.write(json.dumps(categories))\n",
    "        #np.save('all_terms.npy', np.array(all_terms))\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e1238fc-27da-4875-a91e-2c81d084df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset_arff_tocsv(name, datasets, output_path):\n",
    "    is_GO, train, val, test = datasets[name]\n",
    "    return arff_data_to_csv(train, is_GO, output_path), arff_data_to_csv(val, is_GO, output_path), arff_data_to_csv(test, is_GO, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e64a7232-7b39-452c-9178-6f5d003e001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'seq_FUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08783457-f9ea-49db-8e18-5f934216f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(data_path, 'HMC_data_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ceeaced-aa93-4281-92f1-afb6ef6bfa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = initialize_dataset_arff_tocsv(dataset_name, datasets, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27ce2e90-273c-4821-a175-7692daba5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/train.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/val.csv\n",
      "CSV salvo em: /home/bruno/git/hmc-torch/data/HMC_data_csv/test.csv\n"
     ]
    }
   ],
   "source": [
    "train.to_csv(os.path.join(output_path, 'train.csv'))\n",
    "val.to_csv(os.path.join(output_path, 'val.csv'))\n",
    "test.to_csv(os.path.join(output_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4740b6a6-b2ef-4f8f-b46a-4f6f3e2d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/bruno/git/hmc-torch/data/HMC_data_csv/test.csv',  sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23cfe3cd-5213-4a66-87e3-a51ee89bef16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4.8, 0.7, 3.1, 3.6, 3.9, 5.5, 1.4, 6.0, 8.0, ...</td>\n",
       "      <td>01.03.16@10.01.05@10.01.09.03@11.04.03.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5.2, 0.5, 3.6, 2.3, 6.5, 6.5, 1.4, 7.7, 7.9, ...</td>\n",
       "      <td>10.01.05.03.05@10.01.09.03@11.04.03.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4.3, 1.1, 2.5, 2.2, 5.7, 6.3, 2.4, 9.2, 6.0, ...</td>\n",
       "      <td>10.01.05.03.05@10.01.09.03@11.04.03.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5.7, 1.0, 2.1, 1.3, 9.4, 6.5, 3.4, 10.4, 1.8,...</td>\n",
       "      <td>02.11@02.13.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2.9, 1.2, 3.3, 2.3, 5.4, 5.0, 1.9, 11.4, 8.9,...</td>\n",
       "      <td>11.04.03.01@16.03.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>[3.2, 0.0, 7.6, 8.3, 2.5, 4.5, 1.3, 7.6, 5.1, ...</td>\n",
       "      <td>11.02.03.01@11.02.03.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>[8.1, 1.4, 6.0, 4.6, 5.1, 4.9, 2.3, 5.7, 4.0, ...</td>\n",
       "      <td>14.07.11.01@14.13.01.01@18.02.01.01.01@20.09.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>[3.5, 1.2, 2.3, 8.1, 5.8, 7.0, 2.3, 4.7, 4.7, ...</td>\n",
       "      <td>11.04.03.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>[2.6, 0.5, 5.8, 5.4, 3.3, 4.9, 3.0, 5.3, 5.8, ...</td>\n",
       "      <td>14.04@14.07.11.01@14.13.01@16.01@20.09.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>[4.5, 0.6, 9.6, 5.8, 6.4, 5.8, 1.3, 5.8, 9.0, ...</td>\n",
       "      <td>10.01.09.05@14.07.04@42.10.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               features  \\\n",
       "0     [4.8, 0.7, 3.1, 3.6, 3.9, 5.5, 1.4, 6.0, 8.0, ...   \n",
       "1     [5.2, 0.5, 3.6, 2.3, 6.5, 6.5, 1.4, 7.7, 7.9, ...   \n",
       "2     [4.3, 1.1, 2.5, 2.2, 5.7, 6.3, 2.4, 9.2, 6.0, ...   \n",
       "3     [5.7, 1.0, 2.1, 1.3, 9.4, 6.5, 3.4, 10.4, 1.8,...   \n",
       "4     [2.9, 1.2, 3.3, 2.3, 5.4, 5.0, 1.9, 11.4, 8.9,...   \n",
       "...                                                 ...   \n",
       "1334  [3.2, 0.0, 7.6, 8.3, 2.5, 4.5, 1.3, 7.6, 5.1, ...   \n",
       "1335  [8.1, 1.4, 6.0, 4.6, 5.1, 4.9, 2.3, 5.7, 4.0, ...   \n",
       "1336  [3.5, 1.2, 2.3, 8.1, 5.8, 7.0, 2.3, 4.7, 4.7, ...   \n",
       "1337  [2.6, 0.5, 5.8, 5.4, 3.3, 4.9, 3.0, 5.3, 5.8, ...   \n",
       "1338  [4.5, 0.6, 9.6, 5.8, 6.4, 5.8, 1.3, 5.8, 9.0, ...   \n",
       "\n",
       "                                                 labels  \n",
       "0             01.03.16@10.01.05@10.01.09.03@11.04.03.01  \n",
       "1                10.01.05.03.05@10.01.09.03@11.04.03.01  \n",
       "2                10.01.05.03.05@10.01.09.03@11.04.03.01  \n",
       "3                                        02.11@02.13.03  \n",
       "4                                  11.04.03.01@16.03.03  \n",
       "...                                                 ...  \n",
       "1334                            11.02.03.01@11.02.03.04  \n",
       "1335  14.07.11.01@14.13.01.01@18.02.01.01.01@20.09.0...  \n",
       "1336                                        11.04.03.01  \n",
       "1337          14.04@14.07.11.01@14.13.01@16.01@20.09.13  \n",
       "1338                      10.01.09.05@14.07.04@42.10.03  \n",
       "\n",
       "[1339 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb0242-5e46-4ac6-bb79-63542aa5e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dump = json.dumps(train.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12666c9d-7247-4198-9d1b-eb897bceee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['features'] = df['features'].apply(json.loads)  # Agora funcionará\n",
    "\n",
    "X = df['features'].tolist()  # Se quiser lista\n",
    "# ou diretamente em NumPy\n",
    "X = np.array(df['features'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "494f97d7-3167-4cb5-8b3d-6f8dfd048483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.8, 0.7, 3.1, ..., 0. , 0. , 1. ],\n",
       "       [5.2, 0.5, 3.6, ..., 0. , 0. , 1. ],\n",
       "       [4.3, 1.1, 2.5, ..., 0. , 0. , 1. ],\n",
       "       ...,\n",
       "       [3.5, 1.2, 2.3, ..., 0. , 1. , 0. ],\n",
       "       [2.6, 0.5, 5.8, ..., 0. , 1. , 0. ],\n",
       "       [4.5, 0.6, 9.6, ..., 0. , 1. , 0. ]], shape=(1339, 529))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320ea56-2d02-487d-ba27-ff1a0117ece5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
