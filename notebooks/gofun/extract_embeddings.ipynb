{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc2d3d4-d962-49b5-b8f1-272c6cee8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precompute_embeddings.py\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3608ddad-6d25-4407-8873-7d157df335fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c816b4b-04fe-4937-87fb-54c9221d1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b456b021-a327-47de-95d5-01dabe6772f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22523fd121bf4ce08a0dc52c28cfdc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae53f550-9695-4fa4-9d9d-3a8cea46185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_roundtrip_for_sequence(seq: str, out_dir: str, protein_id: str, num_steps: int = 8):\n",
    "    \"\"\"\n",
    "    Roda: completar sequência (se tiver máscaras) -> estrutura -> round-trip\n",
    "    e salva dois PDBs: generation e round_tripped.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    protein = ESMProtein(sequence=seq)\n",
    "\n",
    "    # 1) Completar / gerar sequência (se tiver '_' como mask)\n",
    "    protein = model.generate(\n",
    "        protein,\n",
    "        GenerationConfig(track=\"sequence\", num_steps=num_steps, temperature=0.7),\n",
    "    )\n",
    "\n",
    "    # 2) Gerar estrutura para essa sequência\n",
    "    protein = model.generate(\n",
    "        protein,\n",
    "        GenerationConfig(track=\"structure\", num_steps=num_steps),\n",
    "    )\n",
    "    gen_pdb_path = out_dir / f\"{protein_id}_generation.pdb\"\n",
    "    protein.to_pdb(str(gen_pdb_path))\n",
    "\n",
    "    # 3) Round-trip: apagar sequência, re-gerar, depois apagar coords e re-foldar\n",
    "    protein.sequence = None\n",
    "    protein = model.generate(\n",
    "        protein,\n",
    "        GenerationConfig(track=\"sequence\", num_steps=num_steps),\n",
    "    )\n",
    "\n",
    "    protein.coordinates = None\n",
    "    protein = model.generate(\n",
    "        protein,\n",
    "        GenerationConfig(track=\"structure\", num_steps=num_steps),\n",
    "    )\n",
    "    rt_pdb_path = out_dir / f\"{protein_id}_round_tripped.pdb\"\n",
    "    protein.to_pdb(str(rt_pdb_path))\n",
    "\n",
    "    return str(gen_pdb_path), str(rt_pdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23de908-6964-4db3-a98a-0f263b8d8975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a6a809401140809ea87f33b08c4148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/Projects/git/hmc-torch/.venv/lib/python3.12/site-packages/esm/pretrained.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# This will download the model weights and instantiate the model on your machine.\n",
    "model = ESM3.from_pretrained(\"esm3-open\").to(\"cuda\") # or \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5bb98e-6325-4232-b24e-a1fd96d8742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"./esm3-open.pth\")  # salva o objeto inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6c39db-e968-4fa3-8132-ea37c13c061c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  9.20it/s]\n",
      "/home/bruno/Projects/git/hmc-torch/.venv/lib/python3.12/site-packages/esm/pretrained.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "/home/bruno/Projects/git/hmc-torch/.venv/lib/python3.12/site-packages/esm/pretrained.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 12.54it/s]\n",
      "/home/bruno/Projects/git/hmc-torch/.venv/lib/python3.12/site-packages/esm/utils/structure/protein_complex.py:298: UserWarning: Entity ID not found in metadata, using None as default\n",
      "  warnings.warn(\"Entity ID not found in metadata, using None as default\")\n",
      "/home/bruno/Projects/git/hmc-torch/.venv/lib/python3.12/site-packages/esm/pretrained.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n",
      "/home/bruno/Projects/git/hmc-torch/.venv/lib/python3.12/site-packages/esm/models/vqvae.py:283: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):  # type: ignore\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 12.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 11.87it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a completion for a partial Carbonic Anhydrase (2vvb)\n",
    "prompt = \"___________________________________________________DQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPP___________________________________________________________\"\n",
    "protein = ESMProtein(sequence=prompt)\n",
    "# Generate the sequence, then the structure. This will iteratively unmask the sequence track.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7))\n",
    "# We can show the predicted structure for the generated sequence.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "protein.to_pdb(\"./generation.pdb\")\n",
    "# Then we can do a round trip design by inverse folding the sequence and recomputing the structure\n",
    "protein.sequence = None\n",
    "protein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8))\n",
    "protein.coordinates = None\n",
    "protein = model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "protein.to_pdb(\"./round_tripped.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70e371-1102-4098-a90e-b03fbd4c20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: substitua isso pela sua forma real de carregar o dataset\n",
    "# Ex.: ler de um TSV, FASTA, etc.\n",
    "sequences = [\n",
    "    # (\"protein_id_1\", \"MKT...\"),\n",
    "    # (\"protein_id_2\", \"AGT...\"),\n",
    "]\n",
    "\n",
    "N = len(sequences)\n",
    "if N == 0:\n",
    "    raise RuntimeError(\"A lista 'sequences' está vazia. Carregue suas sequências aqui.\")\n",
    "\n",
    "# Dimensão da embedding (depende do modelo)\n",
    "embed_dim = model.config.hidden_size\n",
    "\n",
    "# Cria memmap para salvar tudo em disco\n",
    "emb_path = \"./embed_protein.dat\"\n",
    "emb_memmap = np.memmap(\n",
    "    emb_path,\n",
    "    dtype=\"float32\",\n",
    "    mode=\"w+\",\n",
    "    shape=(N, embed_dim),\n",
    ")\n",
    "\n",
    "batch_size = 8  # ajuste conforme sua GPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    for start in tqdm(range(0, N, batch_size), desc=\"Gerando embeddings\"):\n",
    "        end = min(start + batch_size, N)\n",
    "        batch_seqs = [seq for _, seq in sequences[start:end]]\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            batch_seqs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "        # Exemplo genérico: usar last_hidden_state (B, L, D)\n",
    "        # Verifique na doc do modelo se existe um pooled output mais adequado.\n",
    "        reps = outputs.last_hidden_state  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        # Máscara de atenção para ignorar padding na média\n",
    "        attn_mask = tokens[\"attention_mask\"].unsqueeze(-1)  # (B, L, 1)\n",
    "        reps = reps * attn_mask\n",
    "        lengths = attn_mask.sum(dim=1).clamp(min=1)  # (B, 1)\n",
    "        emb_batch = reps.sum(dim=1) / lengths       # (B, D)\n",
    "\n",
    "        emb_batch = emb_batch.cpu().numpy().astype(\"float32\")\n",
    "        emb_memmap[start:end, :] = emb_batch\n",
    "\n",
    "emb_memmap.flush()\n",
    "print(f\"Salvo em {emb_path} com shape {(N, embed_dim)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
